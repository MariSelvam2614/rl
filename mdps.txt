import numpy as np
states = [0, 1, 2, 3]
actions = ['left', 'right']
transition_probs = {
0: {'left': [(1.0, 0, 0)], 'right': [(1.0, 1, 0)]},
1: {'left': [(1.0, 0, 0)], 'right': [(1.0, 2, 0)]},
2: {'left': [(1.0, 1, 0)], 'right': [(1.0, 3, 1)]},
3: {'left': [(1.0, 3, 0)], 'right': [(1.0, 3, 0)]},
}
gamma = 0.9
theta = 1e-6
V = np.zeros(len(states))
def value_iteration():
while True:
delta = 0
for s in states:
if s == 3:
continue
v = V[s]
action_values = []
for a in actions:
total = 0
for (prob, next_state, reward) in transition_probs[s][a]:
total += prob * (reward + gamma * V[next_state])
action_values.append(total)
V[s] = max(action_values)
delta = max(delta, abs(v - V[s]))
if delta < theta:
break
return V
def extract_policy(V):
policy = {}
for s in states:
if s == 3:
policy[s] = 'exit'
continue
action_values = {}
for a in actions:
total = 0
for (prob, next_state, reward) in transition_probs[s][a]:
total += prob * (reward + gamma * V[next_state])
action_values[a] = total
best_action = max(action_values, key=action_values.get)
policy[s] = best_action
return policy
optimal_values = value_iteration()
optimal_policy = extract_policy(optimal_values)
print("Optimal State Values:")
for i, v in enumerate(optimal_values):
print(f"V({i}) = {v:.4f}")
print("\nOptimal Policy:")
for s in optimal_policy:
print(f"?({s}) = {optimal_policy[s]}")
