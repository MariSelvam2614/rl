{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueIhvvwSU-GN",
        "outputId": "f7c5a915-7326-4473-caf3-a9c2e22325fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Value function from Monte Carlo Prediction:\n",
            "State 0: V = 0.0044\n",
            "State 1: V = 0.0035\n",
            "State 2: V = 0.0077\n",
            "State 3: V = 0.0006\n",
            "State 4: V = 0.0061\n",
            "State 6: V = 0.0205\n",
            "State 8: V = 0.0166\n",
            "State 9: V = 0.0477\n",
            "State 10: V = 0.0789\n",
            "State 13: V = 0.1359\n",
            "State 14: V = 0.3506\n",
            "\n",
            "Learned Policy (0=Left, 1=Down, 2=Right, 3=Up):\n",
            "State 0: Best Action = 2\n",
            "State 1: Best Action = 2\n",
            "State 2: Best Action = 0\n",
            "State 3: Best Action = 0\n",
            "State 4: Best Action = 3\n",
            "State 5: Best Action = 2\n",
            "State 6: Best Action = 2\n",
            "State 7: Best Action = 3\n",
            "State 8: Best Action = 3\n",
            "State 9: Best Action = 1\n",
            "State 10: Best Action = 1\n",
            "State 11: Best Action = 3\n",
            "State 12: Best Action = 1\n",
            "State 13: Best Action = 1\n",
            "State 14: Best Action = 2\n",
            "State 15: Best Action = 2\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "from collections import defaultdict\n",
        "# Fix np.bool8 deprecation if needed\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_\n",
        "# Create environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "# -------------------------------------\n",
        "# Monte Carlo Prediction\n",
        "# -------------------------------------\n",
        "def mc_prediction(policy, env, episodes=5000, gamma=0.9):\n",
        "    value_table = defaultdict(float)\n",
        "    returns = defaultdict(list)\n",
        "    for _ in range(episodes):\n",
        "        # env.reset() now returns a tuple: (observation, info)\n",
        "        state, info = env.reset()\n",
        "        episode = []\n",
        "        while True:\n",
        "            action = policy(state)\n",
        "            # env.step() now returns a tuple: (observation, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated # Check for both terminated and truncated\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state_t, _, reward_t = episode[t]\n",
        "            G = gamma * G + reward_t\n",
        "            if state_t not in visited:\n",
        "                returns[state_t].append(G)\n",
        "                value_table[state_t] = np.mean(returns[state_t])\n",
        "                visited.add(state_t)\n",
        "    return value_table\n",
        "def random_policy(state):\n",
        "    return env.action_space.sample()\n",
        "v = mc_prediction(random_policy, env)\n",
        "print(\"Value function from Monte Carlo Prediction:\")\n",
        "for s in sorted(v):\n",
        "    print(f\"State {s}: V = {v[s]:.4f}\")\n",
        "# -------------------------------------\n",
        "# Monte Carlo Control (Îµ-greedy)\n",
        "# -------------------------------------\n",
        "def mc_control_epsilon_greedy(env, episodes=10000, gamma=0.9, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    returns = defaultdict(list)\n",
        "    # Initialize policy with random actions for each state\n",
        "    policy = defaultdict(lambda: np.random.choice(env.action_space.n))\n",
        "    for _ in range(episodes):\n",
        "        # env.reset() now returns a tuple: (observation, info)\n",
        "        state, info = env.reset()\n",
        "        episode = []\n",
        "        while True:\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "            # env.step() now returns a tuple: (observation, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated # Check for both terminated and truncated\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        G = 0\n",
        "        visited_sa = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state_t, action_t, reward_t = episode[t]\n",
        "            G = gamma * G + reward_t\n",
        "            if (state_t, action_t) not in visited_sa:\n",
        "                returns[(state_t, action_t)].append(G)\n",
        "                Q[state_t][action_t] = np.mean(returns[(state_t, action_t)])\n",
        "                # Update the policy for the current state based on the best action\n",
        "                policy[state_t] = np.argmax(Q[state_t])\n",
        "                visited_sa.add((state_t, action_t))\n",
        "    return Q, policy\n",
        "Q, learned_policy = mc_control_epsilon_greedy(env)\n",
        "print(\"\\nLearned Policy (0=Left, 1=Down, 2=Right, 3=Up):\")\n",
        "# Print the policy for each state\n",
        "for s in range(env.observation_space.n):\n",
        "    print(f\"State {s}: Best Action = {learned_policy[s]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXqEuTp4VA3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}